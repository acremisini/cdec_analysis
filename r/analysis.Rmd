---
title: "analysis"
author: "Andres Cremisini"
date: "March 26, 2019"
output: html_document
---

Specify data locations
```{r}
train.path = "../python/data/WDEC_srl_as_train.csv"
test.path = "../python/data/WDEC_srl_as_test.csv"
```

Load and prepare data
```{r}
library(caret)
library(DMwR)
library(forcats)
set.seed(123)

# read data to csv
train = read.csv(file=train.path, header=TRUE, sep=",")
test = read.csv(file=test.path, header=TRUE, sep=",")


# clean
train = na.omit(train)
test = na.omit(test)

# make dependent variable a factor
train$Class = as.factor(train$coreferent)
test$Class = as.factor(test$coreferent)

#train$same_sentence = as.factor(train$same_sentence)
#test$same_sentence = as.factor(test$same_sentence)

# change levels from 0,1 to False,True
#levels(train$Class) = c("false", "true")
#levels(test$Class) = c("false", "true")

train$Class = fct_relevel(train$Class, "true")
test$Class = fct_relevel(test$Class, "true")

# remove old response variable
train = subset(train, select=-c(coreferent,same_sentence))
test = subset(test, select=-c(coreferent,same_sentence))
# store response for convenience
train.Y=train$Class
test.Y=test$Class

# under and over sample
train.down_sample = downSample(x = train[, -ncol(train)],
                        y = train$Class)
#train.smote <- SMOTE(Class ~ ., data  = train)   


```
Explore data
```{r}
plot(test$Class,(test$word2vec.similarity)*(test$lemma.overlap+.5)*test$sts)
```


Vanilla Random Forest
```{r}
library(caret)
library(pROC)
library(DMwR)
#library(mlr)
set.seed(123)


fit.RF = train(
               Class ~ ., # define the model
               data = train, # data
               method = "rf",
               metric= "AUC",
               tuneGrid = data.frame(mtry=4),#data.frame(mtry=10), #expand.grid(.mtry=c(4:16)),
               ntree = 100,
               trControl = trainControl(method = "repeatedcv", # cross-validation,
                                        number= 10,
                                        repeats = 5,
                                        summaryFunction = prSummary,
                                        classProbs = TRUE
                                       )
              )

cat("\n---------- Training ------------\n\n")
fit.RF
#0.21631579
preds = predict(fit.RF, test)

# for within sentence nonSrl
#preds = setThreshold(preds, threshold = 0.21631579)
pred_probs = predict(fit.RF, test,type="prob")

cat("\n---------- Testing ------------\n\n")
confusionMatrix(preds, test.Y, positive="true", mode="everything")

dat = data.frame(obs=test.Y,
                 pred=preds,
                 true=pred_probs$true,
                 false=pred_probs$false)

prSummary(dat, lev=levels(test.Y))
twoClassSummary(dat, lev = levels(test.Y))


```
Gradient boosting
```{r}
library(xgboost)
library(caret)
library(plyr)
library(dplyr)

train.matrix = xgb.DMatrix(data = model.matrix(Class ~.,data=train))
test.matrix = xgb.DMatrix(data = model.matrix(Class ~.,data=test))

xgb_trcontrol = trainControl(
                              method = "repeatedcv",
                              number = 10,
                              repeats=5,
                              allowParallel = TRUE,
                              verboseIter = FALSE,
                              returnData = FALSE
                            )

xgbGrid <- expand.grid(nrounds = 300,  # c(100,200),# this is n_estimators in the python code above
                       max_depth = 20,#c(10, 15, 20, 25),
                       colsample_bytree = .5,#seq(0.5, 0.9, length.out = 5),
                       ## The values below are default values in the sklearn-api. 
                       eta = 0.1,#c(0,0.1,0.5,0.9), # 0.1
                       gamma= 0,#c(0,0.1,0.5,0.9), # 0
                       min_child_weight = 1,
                       subsample = 1
                      )
fit.xgb = train(
                  train.matrix, 
                  train.Y,  
                  trControl = trainControl(method = "repeatedcv", # cross-validation,
                                            number= 10,
                                            repeats = 5,
                                            summaryFunction = prSummary,
                                            classProbs = TRUE
                                           ),
                  tuneGrid = xgbGrid,
                  method = "xgbTree",
                  metric="AUC"
                )

cat("\n---------- Training ------------\n\n")
fit.xgb

preds = predict(fit.xgb, test.matrix)
pred_probs = predict(fit.xgb, test.matrix ,type="prob")

cat("\n---------- Testing ------------\n\n")
confusionMatrix(preds, test.Y, positive="true", mode="everything")

dat = data.frame(obs=test.Y,
                 pred=preds,
                 true=pred_probs$true,
                 false=pred_probs$false)

prSummary(dat, lev=levels(test.Y))
twoClassSummary(dat, lev = levels(test.Y))



```

SVM
```{r}
library(caret)
library(pROC)
set.seed(123)

fit.svm = train(
               Class ~., # define the model
               data = train, # data
               method = "svmRadial",
               metric = "ROC",
               tuneLength=10,
               preProc = c("center", "scale"),
               trControl = trainControl(method = "repeatedcv", # cross-validation
                                        number= 10,
                                        repeats = 5,
                                        summaryFunction = twoClassSummary,
                                        classProbs = TRUE
                                       )
              )

getTrainPerf(fit.svm)

print(fit.svm)

preds = predict(fit.svm, test)
confusionMatrix(preds, test.Y, positive="True", mode="everything")

fit.svm.roc = roc(test.Y,
                 predict(fit.svm,test, type="prob")[,1],
                 levels=rev(levels(test.Y)))
fit.svm.roc

plot(fit.svm.roc, 
     print.thres = c(.5), 
     type = "S",
     print.thres.pattern = "%.3f (Spec = %.2f, Sens = %.2f)",
     print.thres.cex = .8,
     legacy.axes=TRUE)


```

Random Forest with optimized threshold
```{r}
library(caret)
library(randomForest)
set.seed(123)
## Now customize model to find best threshold

## Get the model code for the original random forest method:

thresh_code <- getModelInfo("rf", regex = FALSE)[[1]]
thresh_code$type <- c("Classification")
## Add the threshold as another tuning parameter
thresh_code$parameters <- data.frame(parameter = c("mtry", "threshold"),
                                     class = c("numeric", "numeric"),
                                     label = c("#Randomly Selected Predictors",
                                               "Probability Cutoff"))
## The default tuning grid code:
thresh_code$grid <- function(x, y, len = NULL, search = "grid") {
  p <- ncol(x)
  if(search == "grid") {
    grid <- expand.grid(mtry = floor(sqrt(p)), #seq(1,p),#mtry = floor(sqrt(p)), # 12 for all was optimal
                        threshold = seq(.01, .99, length = len))
    } else {
      grid <- expand.grid(mtry = sample(1:p, size = len),
                          threshold = runif(1, 0, size = len))
      }
  grid
  }

## Here we fit a single random forest model (with a fixed mtry)
## and loop over the threshold values to get predictions from the same
## randomForest model.
thresh_code$loop = function(grid) {   
  library(plyr)
  loop <- ddply(grid, c("mtry"),
                function(x) c(threshold = max(x$threshold)))
  submodels <- vector(mode = "list", length = nrow(loop))
  for(i in seq(along = loop$threshold)) {
    index <- which(grid$mtry == loop$mtry[i])
    cuts <- grid[index, "threshold"] 
    submodels[[i]] <- data.frame(threshold = cuts[cuts != loop$threshold[i]])
    }    
  list(loop = loop, submodels = submodels)
  }

## Fit the model independent of the threshold parameter
thresh_code$fit = function(x, y, wts, param, lev, last, classProbs, ...) { 
  if(length(levels(y)) != 2)
    stop("This works only for 2-class problems")
  randomForest(x, y, mtry = param$mtry, ...)
  }

## Now get a probability prediction and use different thresholds to
## get the predicted class
thresh_code$predict = function(modelFit, newdata, submodels = NULL) {
  class1Prob <- predict(modelFit, 
                        newdata, 
                        type = "prob")[, modelFit$obsLevels[1]]
  ## Raise the threshold for class #1 and a higher level of
  ## evidence is needed to call it class 1 so it should 
  ## decrease sensitivity and increase specificity
  out <- ifelse(class1Prob >= modelFit$tuneValue$threshold,
                modelFit$obsLevels[1], 
                modelFit$obsLevels[2])
  if(!is.null(submodels)) {
    tmp2 <- out
    out <- vector(mode = "list", length = length(submodels$threshold))
    out[[1]] <- tmp2
    for(i in seq(along = submodels$threshold)) {
      out[[i+1]] <- ifelse(class1Prob >= submodels$threshold[[i]],
                           modelFit$obsLevels[1], 
                           modelFit$obsLevels[2])
      }
    } 
  out  
  }

## The probabilities are always the same but we have to create
## mulitple versions of the probs to evaluate the data across
## thresholds
thresh_code$prob = function(modelFit, newdata, submodels = NULL) {
  out <- as.data.frame(predict(modelFit, newdata, type = "prob"))
  if(!is.null(submodels)) {
    probs <- out
    out <- vector(mode = "list", length = length(submodels$threshold)+1)
    out <- lapply(out, function(x) probs)
    } 
  out 
}

####################

fourStats <- function (data, lev = levels(data$obs), model = NULL) {
  ## This code will get use the area under the ROC curve and the
  ## sensitivity and specificity values using the current candidate
  ## value of the probability threshold.
  out <- c(prSummary(data, lev = levels(data$obs), model = NULL))
  
  ## The best possible model has sensitivity of 1 and specificity of 1. 
  ## How far are we from that value?
  coords <- matrix(c(1, 1, out["Recall"], out["Precision"]), 
                   ncol = 2, 
                   byrow = TRUE)
  colnames(coords) <- c("Recall", "Precision")
  rownames(coords) <- c("Best", "Current")
  c(out, Dist = dist(coords)[1])
}

```
Use above to fit a model
```{r}
library(randomForest)
library(reshape2)
library(pROC)
library(plyr)
library(dplyr)
set.seed(123)

fit.RF.thresh <- train(
              Class ~., 
              data = train,
              method = thresh_code,
              ## Maximize the distance to the perfect model
              metric = "F",
              maximize = TRUE,
              preProc = c("center", "scale"),
              tuneLength = 20,
              ntree = 100,
              trControl = trainControl(method = "repeatedcv",
                                       number=10,
                                       repeats = 5,
                                       classProbs = TRUE,
                                       summaryFunction = fourStats))
fit.RF.thresh
getTrainPerf(fit.RF.thresh)

table(test.Y)
preds = predict(fit.RF.thresh, test)

confusionMatrix(preds, test.Y, positive="true", mode="everything")

fit.RF.thresh.roc = roc(test.Y,
                        predict(fit.RF.thresh,test, type="prob")[,1],
                        levels=rev(levels(test.Y)))



plot(fit.RF.thresh.roc, 
     print.thres = c(.5), 
     type = "S",
     print.thres.pattern = "%.3f (Recall = %.2f, Precision = %.2f)",
     print.thres.cex = .8,
     legacy.axes=TRUE)

metrics <- fit.RF.thresh$results[, c(2, 4:6)]
metrics <- melt(metrics, id.vars = "threshold", 
                variable.name = "Resampled",
                value.name = "Data")

ggplot(metrics, aes(x = threshold, y = Data, color = Resampled)) + 
  geom_line() + 
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top")
```


Lasso
```{r}
library(caret)
library(pROC)
set.seed(123)

# fit a lasso model 
fit.lasso = train(
                  Class ~., # define the model
                  data = train, # data
                  method = "glmnet", # use glmnet
                  family="binomial", # logistic regression
                  trControl = trainControl(method = "repeatedcv", # cross-validation,
                                           repeats = 5,
                                           summaryFunction = twoClassSummary,
                                           classProbs = TRUE
                                          ),
                  metric = "ROC", # optimize by ROC
                  tuneGrid=expand.grid(.alpha=1, .lambda=seq(0, 1, by = 0.1)) # grid for lambda search
                  )
# print results

getTrainPerf(fit.lasso)
coef(fit.lasso$finalModel,fit.lasso$bestTune$.lambda)
fit.lasso.roc = roc(test.Y,
                    predict(fit.lasso,test, type="prob")[,1],
                    levels=rev(levels(test.Y)))
fit.lasso.roc
plot(fit.lasso.roc, 
     print.thres = c(.5), 
     type = "S",
     print.thres.pattern = "%.3f (Spec = %.2f, Sens = %.2f)",
     print.thres.cex = .8,
     legacy.axes=TRUE)



```
